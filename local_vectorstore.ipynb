{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.human import HumanInputLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Agent\n",
    "import langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(langchain.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m _DEFAULT_TEMPLATE \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39mRelevant pieces of previous conversation:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mHuman: \u001b[39m\u001b[39m{input}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mAI:\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m PROMPT \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m     13\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m], template\u001b[39m=\u001b[39m_DEFAULT_TEMPLATE\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m conversation_with_summary \u001b[39m=\u001b[39m ConversationChain(\n\u001b[1;32m     16\u001b[0m     llm\u001b[39m=\u001b[39mllm, \n\u001b[1;32m     17\u001b[0m     prompt\u001b[39m=\u001b[39mPROMPT,\n\u001b[1;32m     18\u001b[0m     \u001b[39m# We set a very low max_token_limit for the purposes of testing.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     memory\u001b[39m=\u001b[39mmemory,\n\u001b[1;32m     20\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m conversation_with_summary\u001b[39m.\u001b[39mpredict(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHi, my name is Perry, what\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms up?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConversationChain' is not defined"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0) # Can be any valid LLM\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "(You do not need to use these pieces of information if not relevant)\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=PROMPT,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, my name is Perry, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "\n",
      "Human: Hi there my friend\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hello there! How can I help you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Hi there my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Hi there my friend\n",
      "AI:  Hello there! How can I help you today?\n",
      "Human: Not too bad - Summarize Feyman's lectures\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Richard Feynman was a Nobel Prize-winning physicist known for his work in quantum mechanics, particle physics, and the philosophy of science. He was also a renowned teacher and lecturer, and his lectures on physics are considered some of the most influential of all time. His lectures covered topics like quantum mechanics, relativity, electromagnetism, thermodynamics, and more. He was known for his unique approach to teaching, which emphasized the importance of understanding the underlying principles of physics rather than simply memorizing equations.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Not too bad - Summarize Feyman's lectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Metro_zori_sm_month.csv') as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/real_estate_data/lib/python3.9/site-packages/langchain/embeddings/openai.py:194\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     tokens \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[0;32m----> 3\u001b[0m docsearch \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_texts(texts, embeddings, metadatas\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m: i} \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(texts))])\n",
      "File \u001b[0;32m~/anaconda3/envs/real_estate_data/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:390\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[39mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    384\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m    385\u001b[0m     embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[1;32m    389\u001b[0m )\n\u001b[0;32m--> 390\u001b[0m chroma_collection\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[1;32m    391\u001b[0m \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/anaconda3/envs/real_estate_data/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:159\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(\u001b[39mlist\u001b[39;49m(texts))\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collection\u001b[39m.\u001b[39madd(\n\u001b[1;32m    161\u001b[0m     metadatas\u001b[39m=\u001b[39mmetadatas, embeddings\u001b[39m=\u001b[39membeddings, documents\u001b[39m=\u001b[39mtexts, ids\u001b[39m=\u001b[39mids\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/anaconda3/envs/real_estate_data/lib/python3.9/site-packages/langchain/embeddings/openai.py:289\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/anaconda3/envs/real_estate_data/lib/python3.9/site-packages/langchain/embeddings/openai.py:250\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n\u001b[1;32m    249\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to for OpenAIEmbeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (23.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting mosaicml[nlp,streaming]==0.10.1\n",
      "  Downloading mosaicml-0.10.1-py3-none-any.whl (612 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=6.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (6.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.62.3 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (4.65.0)\n",
      "Collecting torchmetrics<0.8,>=0.7.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.2/398.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-optimizer<0.4,>=0.3.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (0.15.2)\n",
      "Collecting torch<2,>=1.10 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading torch-1.13.1-cp39-none-macosx_11_0_arm64.whl (53.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting yahp<0.2,>=0.1.3 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading yahp-0.1.3-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m250.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.26.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (2.29.0)\n",
      "Collecting numpy<1.23.0,>=1.21.5 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading numpy-1.22.4-cp39-cp39-macosx_11_0_arm64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil<6,>=5.8.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (5.9.5)\n",
      "Collecting coolname<2,>=1.1.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading coolname-1.1.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tabulate==0.8.10 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting py-cpuinfo<9,>=8.0.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m423.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting packaging<22,>=21.3.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata<5,>=4.11.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: transformers<5,>=4.11 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (4.29.2)\n",
      "Collecting datasets<3,>=2.4.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mosaicml-streaming (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached mosaicml_streaming-0.4.1-py3-none-any.whl (148 kB)\n",
      "Requirement already satisfied: boto3<2,>=1.21.45 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from mosaicml[nlp,streaming]==0.10.1) (1.26.136)\n",
      "Collecting paramiko<3,>=2.11.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: botocore<1.30.0,>=1.29.136 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from boto3<2,>=1.21.45->mosaicml[nlp,streaming]==0.10.1) (1.29.136)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from boto3<2,>=1.21.45->mosaicml[nlp,streaming]==0.10.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from boto3<2,>=1.21.45->mosaicml[nlp,streaming]==0.10.1) (0.6.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (12.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (2.0.1)\n",
      "Collecting xxhash (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-macosx_11_0_arm64.whl (31 kB)\n",
      "Collecting multiprocess (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (0.14.1)\n",
      "Collecting responses<0.19 (from datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from importlib-metadata<5,>=4.11.0->mosaicml[nlp,streaming]==0.10.1) (3.15.0)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2 (from packaging<22,>=21.3.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting bcrypt>=3.1.3 (from paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-macosx_10_10_universal2.whl (473 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.4/473.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1) (40.0.2)\n",
      "Collecting pynacl>=1.0.1 (from paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl (349 kB)\n",
      "Requirement already satisfied: six in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from requests<3,>=2.26.0->mosaicml[nlp,streaming]==0.10.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from requests<3,>=2.26.0->mosaicml[nlp,streaming]==0.10.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from requests<3,>=2.26.0->mosaicml[nlp,streaming]==0.10.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from requests<3,>=2.26.0->mosaicml[nlp,streaming]==0.10.1) (2023.5.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from torch<2,>=1.10->mosaicml[nlp,streaming]==0.10.1) (4.5.0)\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer<0.4,>=0.3.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting pyDeprecate==0.3.* (from torchmetrics<0.8,>=0.7.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision>=0.10.0 (from mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading torchvision-0.15.1-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchvision-0.14.1-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from torchvision>=0.10.0->mosaicml[nlp,streaming]==0.10.1) (9.5.0)\n",
      "Requirement already satisfied: filelock in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from transformers<5,>=4.11->mosaicml[nlp,streaming]==0.10.1) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from transformers<5,>=4.11->mosaicml[nlp,streaming]==0.10.1) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from transformers<5,>=4.11->mosaicml[nlp,streaming]==0.10.1) (0.13.3)\n",
      "Collecting ruamel.yaml>=0.17.10 (from yahp<0.2,>=0.1.3->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading ruamel.yaml-0.17.26-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m960.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docstring-parser<=0.15,>=0.14.1 (from yahp<0.2,>=0.1.3->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting jsonschema<4.8,>=4.7.2 (from yahp<0.2,>=0.1.3->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading jsonschema-4.7.2-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m788.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Brotli>=1.0.9 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading Brotli-1.0.9-cp39-cp39-macosx_10_9_universal2.whl (786 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.7/786.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib<4,>=3.5.2 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-macosx_11_0_arm64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-snappy<1,>=0.6.1 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading python_snappy-0.6.1-cp39-cp39-macosx_10_9_universal2.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchtext>=0.10 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading torchtext-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting zstd<2,>=1.5.2.5 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading zstd-1.5.5.1-cp39-cp39-macosx_11_0_arm64.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oci<3,>=2.88 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached oci-2.102.0-py3-none-any.whl (20.0 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.136->boto3<2,>=1.21.45->mosaicml[nlp,streaming]==0.10.1) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from cryptography>=2.5->paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1) (1.15.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from aiohttp->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (1.3.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from jsonschema<4.8,>=4.7.2->yahp<0.2,>=0.1.3->mosaicml[nlp,streaming]==0.10.1) (0.19.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4,>=3.5.2->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-macosx_11_0_arm64.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib<4,>=3.5.2->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4,>=3.5.2->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib<4,>=3.5.2->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-macosx_11_0_arm64.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources>=3.2.0 (from matplotlib<4,>=3.5.2->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting cryptography>=2.5 (from paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading cryptography-39.0.2-cp36-abi3-macosx_10_12_universal2.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyOpenSSL<24.0.0,>=17.5.0 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from oci<3,>=2.88->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1) (23.1.1)\n",
      "Requirement already satisfied: pytz>=2016.10 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from oci<3,>=2.88->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1) (2023.3)\n",
      "Collecting circuitbreaker<2.0.0,>=1.3.1 (from oci<3,>=2.88->mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Using cached circuitbreaker-1.4.0.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.10->yahp<0.2,>=0.1.3->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading ruamel.yaml.clib-0.2.7-cp39-cp39-macosx_12_0_arm64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchtext>=0.10 (from mosaicml-streaming->mosaicml[nlp,streaming]==0.10.1)\n",
      "  Downloading torchtext-0.15.1-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchtext-0.14.1-cp39-cp39-macosx_12_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from pandas->datasets<3,>=2.4.0->mosaicml[nlp,streaming]==0.10.1) (2023.3)\n",
      "Requirement already satisfied: pycparser in /Users/lnshuti/anaconda3/envs/real_estate_data/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->paramiko<3,>=2.11.0->mosaicml[nlp,streaming]==0.10.1) (2.21)\n",
      "Building wheels for collected packages: py-cpuinfo, circuitbreaker\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22242 sha256=6bb50c4dec67bd46417ca5deed225b95099b5b337ea328589bfad21a59b468c9\n",
      "  Stored in directory: /Users/lnshuti/Library/Caches/pip/wheels/a9/33/c2/bcf6550ff9c95f699d7b2f261c8520b42b7f7c33b6e6920e29\n",
      "  Building wheel for circuitbreaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuitbreaker: filename=circuitbreaker-1.4.0-py3-none-any.whl size=7518 sha256=010d545a6423a1dcc8309d0c8c6114cdba4ce2ca62ad2accf1534c67e0850af2\n",
      "  Stored in directory: /Users/lnshuti/Library/Caches/pip/wheels/ba/dd/e9/c2cd003b8feaabf3931f90b79905ba1e5171f3002671f380dc\n",
      "Successfully built py-cpuinfo circuitbreaker\n",
      "Installing collected packages: zstd, python-snappy, py-cpuinfo, coolname, circuitbreaker, Brotli, xxhash, torch, tabulate, ruamel.yaml.clib, pyparsing, pyDeprecate, numpy, kiwisolver, jsonschema, importlib-resources, importlib-metadata, fonttools, docstring-parser, dill, cycler, bcrypt, torchvision, torchtext, ruamel.yaml, responses, pytorch-ranger, pynacl, packaging, multiprocess, cryptography, contourpy, yahp, torchmetrics, torch-optimizer, paramiko, matplotlib, oci, mosaicml, datasets, mosaicml-streaming\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.17.3\n",
      "    Uninstalling jsonschema-4.17.3:\n",
      "      Successfully uninstalled jsonschema-4.17.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.6.0\n",
      "    Uninstalling importlib-metadata-6.6.0:\n",
      "      Successfully uninstalled importlib-metadata-6.6.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.2\n",
      "    Uninstalling torchvision-0.15.2:\n",
      "      Successfully uninstalled torchvision-0.15.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 40.0.2\n",
      "    Uninstalling cryptography-40.0.2:\n",
      "      Successfully uninstalled cryptography-40.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "semantic-kernel 0.2.8.dev0 requires numpy<2.0.0,>=1.24.2, but you have numpy 1.22.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Brotli-1.0.9 bcrypt-4.0.1 circuitbreaker-1.4.0 contourpy-1.0.7 coolname-1.1.0 cryptography-39.0.2 cycler-0.11.0 datasets-2.12.0 dill-0.3.6 docstring-parser-0.15 fonttools-4.39.4 importlib-metadata-4.13.0 importlib-resources-5.12.0 jsonschema-4.7.2 kiwisolver-1.4.4 matplotlib-3.7.1 mosaicml-0.10.1 mosaicml-streaming-0.4.1 multiprocess-0.70.14 numpy-1.22.4 oci-2.102.0 packaging-21.3 paramiko-2.12.0 py-cpuinfo-8.0.0 pyDeprecate-0.3.2 pynacl-1.5.0 pyparsing-3.0.9 python-snappy-0.6.1 pytorch-ranger-0.1.1 responses-0.18.0 ruamel.yaml-0.17.26 ruamel.yaml.clib-0.2.7 tabulate-0.8.10 torch-1.13.1 torch-optimizer-0.3.0 torchmetrics-0.7.3 torchtext-0.14.1 torchvision-0.14.1 xxhash-3.2.0 yahp-0.1.3 zstd-1.5.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip\n",
    "%pip install -U 'mosaicml[nlp, streaming]==0.10.1'\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# %pip install 'mosaicml[nlp, tensorboard] @ git+https://github.com/mosaicml/composer.git'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# Create a BERT sequence classification model using Hugging Face transformers\n",
    "config = transformers.AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.AutoModelForMaskedLM.from_config(config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.datasets import StreamingC4\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Tokenize the C4 dataset\n",
    "train_dataset = StreamingC4(remote='s3://mosaicml-internal-temporary-202210-ocwdemo/mds/1-gz', \n",
    "                                    local='/tmp/c4local',\n",
    "                                    shuffle=True,\n",
    "                                    max_seq_len=128,\n",
    "                                    split='train', \n",
    "                                    tokenizer_name='bert-base-uncased')\n",
    "eval_dataset = StreamingC4(remote='s3://mosaicml-internal-temporary-202210-ocwdemo/mds/1-gz',\n",
    "                                    local='/tmp/c4local',\n",
    "                                    shuffle=True,\n",
    "                                    max_seq_len=128,\n",
    "                                    split='val',\n",
    "                                    tokenizer_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "# data_collator = transformers.DefaultDataCollator(return_tensors='pt')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset,batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.collections import MetricCollection\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.metrics import LanguageCrossEntropy, MaskedAccuracy\n",
    "\n",
    "metrics = [LanguageCrossEntropy(vocab_size=tokenizer.vocab_size), MaskedAccuracy(ignore_index=-100)]\n",
    "# Package as a trainer-friendly Composer model\n",
    "composer_model = HuggingFaceModel(model, metrics=metrics, use_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "optimizer = AdamW(\n",
    "    params=composer_model.parameters(),\n",
    "    lr=3e-5, betas=(0.9, 0.98),\n",
    "    eps=1e-6, weight_decay=3e-6\n",
    ")\n",
    "linear_lr_decay = LinearLR(\n",
    "    optimizer, start_factor=1.0,\n",
    "    end_factor=0, total_iters=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=composer_model, # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[linear_lr_decay],\n",
    "    device='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    train_subset_num_batches=150,\n",
    "    eval_subset_num_batches=150,\n",
    "    precision='fp32',\n",
    "    seed=17\n",
    ")\n",
    "# Start training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a BERT sequence classification model using Hugging Face transformers\n",
    "sentiment_model = transformers.AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "sst2_tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Create BERT tokenizer\n",
    "def tokenize_function(sample):\n",
    "    return sst2_tokenizer(\n",
    "        text=sample['sentence'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "# Tokenize SST-2\n",
    "sst2_dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "tokenized_sst2_dataset = sst2_dataset.map(tokenize_function,\n",
    "                                          batched=True, \n",
    "                                          num_proc=cpu_count(),\n",
    "                                          batch_size=100,\n",
    "                                          remove_columns=['idx', 'sentence'])\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "sst2_train_dataset = tokenized_sst2_dataset[\"train\"]\n",
    "sst2_eval_dataset = tokenized_sst2_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "sst2_data_collator = transformers.data.data_collator.default_data_collator\n",
    "sst2_train_dataloader = DataLoader(sst2_train_dataset, batch_size=16, shuffle=False, drop_last=False, collate_fn=sst2_data_collator)\n",
    "sst2_eval_dataloader = DataLoader(sst2_eval_dataset,batch_size=16, shuffle=False, drop_last=False, collate_fn=sst2_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.collections import MetricCollection\n",
    "from composer.metrics import CrossEntropy\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "\n",
    "metrics = [CrossEntropy(), Accuracy()]\n",
    "# Package as a trainer-friendly Composer model\n",
    "composer_sentiment_model = HuggingFaceModel(sentiment_model, metrics=metrics, use_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "sst2_optimizer = AdamW(\n",
    "    params=composer_sentiment_model.parameters(),\n",
    "    lr=3e-5, betas=(0.9, 0.98),\n",
    "    eps=1e-6, weight_decay=3e-6\n",
    ")\n",
    "sst2_linear_lr_decay = LinearLR(\n",
    "    sst2_optimizer, start_factor=1.0,\n",
    "    end_factor=0, total_iters=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "sentiment_trainer = Trainer(\n",
    "    model=composer_sentiment_model, # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=sst2_train_dataloader,\n",
    "    eval_dataloader=sst2_eval_dataloader,\n",
    "    max_duration=\"1ep\",\n",
    "    optimizers=sst2_optimizer,\n",
    "    schedulers=[sst2_linear_lr_decay],\n",
    "    device='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    train_subset_num_batches=150,\n",
    "    eval_subset_num_batches=150,\n",
    "    precision='fp32',\n",
    "    seed=17\n",
    ")\n",
    "# Start training\n",
    "sentiment_trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the eval accuracy metric in the final output, we can see our model reaches ~86% accuracy with only 150 iterations of training! Let's visualize a few samples from the validation set to see how our model performs.\n",
    "\n",
    "We can make our own predictions with the model now. Input your own string and see the sentiment prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play around with this and change this string to your own input!\n",
    "INPUT_STRING = \"Hello, my dog is cute\"\n",
    "\n",
    "input_val = tokenizer(INPUT_STRING, return_tensors=\"pt\")\n",
    "\n",
    "input_batch = {k: v.cuda() if torch.cuda.is_available() else v for k, v in input_val.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = composer_sentiment_model(input_batch).logits\n",
    "    \n",
    "prediction = logits.argmax().item()\n",
    "\n",
    "print(f\"Raw prediction: {prediction}\")\n",
    "\n",
    "label = ['negative', 'positive']\n",
    "\n",
    "print(f\"Sentiment: {label[prediction]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Pre-Trained Model\n",
    "Finally, to save the pre-trained model parameters we call the PyTorch save method and pass it the model's state_dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sentiment_trainer.state.model.state_dict(), 'model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_estate_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
